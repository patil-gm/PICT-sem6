{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783b9b91",
   "metadata": {},
   "source": [
    "# Assignment 10\n",
    "\n",
    "Name : Ghanashyam Patil  \n",
    "Roll No : 31162  \n",
    "Subject : DSBDAL  \n",
    "\n",
    "Problem Statement :\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c588bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0cbb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are importing the txt file for processing\n",
    "file_path = \"patil_gm.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aac20f",
   "metadata": {},
   "source": [
    "# Tockenisation\n",
    "Punkt tokenizer is used to Tokenization in the Natural Language Toolkit (NLTK).  \n",
    "Tokenization is the process of dividing a text into individual units, called tokens, which can be words, sentences, or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86114391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ghanashyam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2883307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tockens :  ['Hello', 'people', '...', '!', 'how', 'is', 'going', 'your', 'preparation', 'of', 'DSBDAL', 'Practicles', 'hope', 'this', 'repository', 'helping', 'you', 'out']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tockens : \",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d4e6032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences :  ['Hello people...!', 'how is going your preparation of DSBDAL Practicles\\nhope this repository helping you out']\n"
     ]
    }
   ],
   "source": [
    "sentences=sent_tokenize(document)\n",
    "print(\"Sentences : \",sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baebde",
   "metadata": {},
   "source": [
    "# Pos Tagging\n",
    "\n",
    "averaged_perceptron_tagger is a part-of-speech (POS) tagger implemented in the Natural Language Toolkit (NLTK) library.  \n",
    "POS tagging is the process of assigning grammatical tags to individual words in a sentence, indicating their syntactic category or part of speech.(like NOUN,PRON,ADJ,VERB, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc12d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('people', 'NNS'), ('...', ':'), ('!', '.'), ('how', 'WRB'), ('is', 'VBZ'), ('going', 'VBG'), ('your', 'PRP$'), ('preparation', 'NN'), ('of', 'IN'), ('DSBDAL', 'NNP'), ('Practicles', 'NNP'), ('hope', 'VBP'), ('this', 'DT'), ('repository', 'NN'), ('helping', 'VBG'), ('you', 'PRP'), ('out', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e48a4",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stopwords typically include common words such as \"a,\" \"an,\" \"the,\" \"is,\" \"and,\" \"of,\" etc.  \n",
    "These words are often removed from text during data preprocessing  \n",
    "to focus on the more important and meaningful words for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb449af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ghanashyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13c7150a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'people', '...', '!', 'going', 'preparation', 'DSBDAL', 'Practicles', 'hope', 'repository', 'helping']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "# stop_words\n",
    "filtered_tokens=[]\n",
    "for token in tokens:\n",
    "    if token not in stop_words:\n",
    "        filtered_tokens.append(token)\n",
    "        \n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0d61a",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "The algorithm follows a series of steps to progressively remove common word endings until the stem is obtained.  \n",
    "e.g. ('running, 'ran', 'runs', 'happiness', 'happier','happiest') converted to\n",
    "     ('run', 'ran', 'run', 'happi', 'happier', 'happiest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "542ee071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'peopl', '...', '!', 'go', 'prepar', 'dsbdal', 'practicl', 'hope', 'repositori', 'help']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda78ce7",
   "metadata": {},
   "source": [
    "# wordnet\n",
    "In natural language processing (NLP), WordNet is a lexical database and resource that provides a structured and organized collection of words and their semantic relationships. It is widely used for various NLP tasks, including word sense disambiguation, synonym identification, semantic analysis, and information retrieval\n",
    "\n",
    "# Lemmatization\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma.  \n",
    "The lemma represents the canonical or uninflected form of a word that conveys its meaning.  \n",
    "The goal of lemmatization is to convert different inflected or derived word forms into a common base form, enabling easier analysis and interpretation of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b25b9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ghanashyam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d20b889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b9638",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse\n",
    "TF-IDF, short for term frequency–inverse document frequency, is a numeric measure that is use to score the importance of a word in a document based on how often did it appear in that document and a given collection of documents. The intuition for this measure is : If a word appears frequently in a document, then it should be important and we should give that word a high score. But if a word appears in too many other documents, it’s probably not a unique identifier, therefore we should assign a lower score to that word\n",
    "\n",
    "# Inverse document frequency\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a145129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF) representation\n",
    "documents = [document]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "311c6cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', 'people', '...', '!', 'how', 'is', 'going', 'your', 'preparation', 'of', 'DSBDAL', 'Practicles', 'hope', 'this', 'repository', 'helping', 'you', 'out']\n",
      "Sentences: ['Hello people...!', 'how is going your preparation of DSBDAL Practicles\\nhope this repository helping you out']\n",
      "POS Tags: [('Hello', 'NNP'), ('people', 'NNS'), ('...', ':'), ('!', '.'), ('how', 'WRB'), ('is', 'VBZ'), ('going', 'VBG'), ('your', 'PRP$'), ('preparation', 'NN'), ('of', 'IN'), ('DSBDAL', 'NNP'), ('Practicles', 'NNP'), ('hope', 'VBP'), ('this', 'DT'), ('repository', 'NN'), ('helping', 'VBG'), ('you', 'PRP'), ('out', 'IN')]\n",
      "Filtered Tokens (after stop words removal): ['Hello', 'people', '...', '!', 'going', 'preparation', 'DSBDAL', 'Practicles', 'hope', 'repository', 'helping']\n",
      "Stemmed Tokens: ['hello', 'peopl', '...', '!', 'go', 'prepar', 'dsbdal', 'practicl', 'hope', 'repositori', 'help']\n",
      "Lemmatized Tokens: ['Hello', 'people', '...', '!', 'going', 'preparation', 'DSBDAL', 'Practicles', 'hope', 'repository', 'helping']\n",
      "TF-IDF Matrix:\n",
      "[[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      "  0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Filtered Tokens (after stop words removal):\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "# print(\"Feature Names:\", vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
